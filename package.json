{
  "name": "@derogab/llm-proxy",
  "description": "A simple and lightweight proxy for seamless integration with multiple LLM providers including OpenAI, Ollama, and Cloudflare AI",
  "version": "0.4.1",
  "type": "module",
  "author": "derogab",
  "license": "MIT",
  "repository": {
    "type": "git",
    "url": "https://github.com/derogab/llm-proxy.git"
  },
  "main": "./dist/cjs/index.js",
  "module": "./dist/esm/index.js",
  "types": "./dist/types/index.d.ts",
  "exports": {
    ".": {
      "import": {
        "types": "./dist/types/index.d.ts",
        "default": "./dist/esm/index.js"
      },
      "require": {
        "types": "./dist/types/index.d.ts",
        "default": "./dist/cjs/index.js"
      }
    }
  },
  "scripts": {
    "build": "npm run build:cjs && npm run build:esm && npm run build:types",
    "build:cjs": "tsc -p tsconfig.cjs.json && echo '{\"type\":\"commonjs\"}' > dist/cjs/package.json",
    "build:esm": "tsc -p tsconfig.esm.json",
    "build:types": "tsc -p tsconfig.types.json",
    "test": "vitest run && npx tsx test/llama-cpp.integration.test.mts",
    "test:unit": "vitest run",
    "test:watch": "vitest",
    "test:coverage": "vitest run --coverage",
    "test:llama": "npx tsx test/llama-cpp.integration.test.mts"
  },
  "files": [
    "dist"
  ],
  "keywords": [
    "LLM",
    "proxy",
    "gateway"
  ],
  "devDependencies": {
    "@types/node": "25.0.6",
    "@vitest/coverage-v8": "4.0.18",
    "typescript": "5.9.3",
    "vitest": "4.0.16"
  },
  "dependencies": {
    "axios": "1.13.2",
    "dotenv": "17.2.3",
    "node-llama-cpp": "3.15.0",
    "ollama": "0.6.3",
    "openai": "6.16.0"
  }
}
