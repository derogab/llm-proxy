{
  "name": "@derogab/llm-proxy",
  "description": "A simple and lightweight proxy for seamless integration with multiple LLM providers including OpenAI, Ollama, and Cloudflare AI",
  "version": "0.3.3",
  "type": "module",
  "author": "derogab",
  "license": "MIT",
  "repository": {
    "type": "git",
    "url": "https://github.com/derogab/llm-proxy.git"
  },
  "main": "./dist/cjs/index.js",
  "module": "./dist/esm/index.js",
  "types": "./dist/types/index.d.ts",
  "exports": {
    ".": {
      "import": {
        "types": "./dist/types/index.d.ts",
        "default": "./dist/esm/index.js"
      },
      "require": {
        "types": "./dist/types/index.d.ts",
        "default": "./dist/cjs/index.js"
      }
    }
  },
  "scripts": {
    "build": "npm run build:cjs && npm run build:esm && npm run build:types",
    "build:cjs": "tsc -p tsconfig.cjs.json",
    "build:esm": "tsc -p tsconfig.esm.json",
    "build:types": "tsc -p tsconfig.types.json"
  },
  "files": [
    "dist"
  ],
  "keywords": [
    "LLM",
    "proxy",
    "gateway"
  ],
  "devDependencies": {
    "@types/node": "24.6.2",
    "typescript": "5.9.3"
  },
  "dependencies": {
    "axios": "1.12.2",
    "dotenv": "17.2.3",
    "node-llama-cpp": "3.14.0",
    "ollama": "0.6.0",
    "openai": "6.1.0"
  }
}
